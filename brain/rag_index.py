# -*- coding: utf-8 -*-
"""
RAG (Retrieval-Augmented Generation) –∏–Ω–¥–µ–∫—Å –¥–ª—è AI –ú–ê–ì–ò–°–¢–†–ê
–õ–æ–∫–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ä–µ–∑—é–º–µ, –ø—Ä–æ–µ–∫—Ç–∞–º, –∫–µ–π—Å–∞–º
"""

import os
import json
import pickle
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import numpy as np
from dataclasses import dataclass
import re


@dataclass
class Document:
    """–î–æ–∫—É–º–µ–Ω—Ç –≤ RAG –∏–Ω–¥–µ–∫—Å–µ"""
    id: str
    title: str
    content: str
    doc_type: str  # "resume", "project", "case_study", "achievement"
    metadata: Dict
    embedding: Optional[np.ndarray] = None
    created_at: str = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now().isoformat()


class SimpleEmbedder:
    """
    –ü—Ä–æ—Å—Ç–æ–π —ç–º–±–µ–¥–¥–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ TF-IDF
    (–í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ sentence-transformers)
    """
    
    def __init__(self):
        self.vocab = {}
        self.idf = {}
        self.doc_count = 0
        
    def fit(self, documents: List[Document]):
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
        """
        self.doc_count = len(documents)
        
        # –°–±–æ—Ä –≤—Å–µ—Ö —Å–ª–æ–≤
        all_words = set()
        for doc in documents:
            words = self._tokenize(doc.content)
            all_words.update(words)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
        self.vocab = {word: idx for idx, word in enumerate(sorted(all_words))}
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ IDF
        for word in self.vocab:
            doc_freq = sum(1 for doc in documents if word in self._tokenize(doc.content))
            self.idf[word] = np.log(self.doc_count / (doc_freq + 1))
    
    def _tokenize(self, text: str) -> List[str]:
        """
        –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        """
        # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        return [word for word in text.split() if len(word) > 2]
    
    def _compute_tf(self, text: str) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ TF –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        """
        words = self._tokenize(text)
        word_count = {}
        for word in words:
            word_count[word] = word_count.get(word, 0) + 1
        
        total_words = len(words)
        tf = {word: count / total_words for word, count in word_count.items()}
        return tf
    
    def embed(self, text: str) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        """
        tf = self._compute_tf(text)
        embedding = np.zeros(len(self.vocab))
        
        for word, tf_score in tf.items():
            if word in self.vocab:
                word_idx = self.vocab[word]
                idf_score = self.idf.get(word, 0)
                embedding[word_idx] = tf_score * idf_score
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
            
        return embedding


class RAGIndex:
    """
    RAG –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
    """
    
    def __init__(self, index_dir: str = "data/rag_index"):
        self.index_dir = index_dir
        self.documents: List[Document] = []
        self.embedder = SimpleEmbedder()
        self.is_trained = False
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs(index_dir, exist_ok=True)
        
    def add_document(self, title: str, content: str, doc_type: str, metadata: Dict = None) -> str:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∏–Ω–¥–µ–∫—Å
        """
        doc_id = f"{doc_type}_{int(datetime.now().timestamp())}"
        
        document = Document(
            id=doc_id,
            title=title,
            content=content,
            doc_type=doc_type,
            metadata=metadata or {}
        )
        
        self.documents.append(document)
        print(f"üìÑ –î–æ–±–∞–≤–ª–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {title}")
        
        return doc_id
    
    def add_resume_section(self, section: str, content: str, metadata: Dict = None):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ —Ä–µ–∑—é–º–µ
        """
        return self.add_document(
            title=f"–†–µ–∑—é–º–µ: {section}",
            content=content,
            doc_type="resume",
            metadata=metadata
        )
    
    def add_project(self, name: str, description: str, tech_stack: List[str], 
                   achievements: List[str], metadata: Dict = None):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
        """
        content = f"""
        –ü—Ä–æ–µ–∫—Ç: {name}
        
        –û–ø–∏—Å–∞–Ω–∏–µ: {description}
        
        –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏: {', '.join(tech_stack)}
        
        –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è:
        {chr(10).join(f"- {achievement}" for achievement in achievements)}
        """
        
        return self.add_document(
            title=f"–ü—Ä–æ–µ–∫—Ç: {name}",
            content=content,
            doc_type="project",
            metadata={
                "tech_stack": tech_stack,
                "achievements": achievements,
                **(metadata or {})
            }
        )
    
    def add_achievement(self, title: str, description: str, metrics: Dict, metadata: Dict = None):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è
        """
        content = f"""
        –î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ: {title}
        
        –û–ø–∏—Å–∞–Ω–∏–µ: {description}
        
        –ú–µ—Ç—Ä–∏–∫–∏:
        {chr(10).join(f"- {key}: {value}" for key, value in metrics.items())}
        """
        
        return self.add_document(
            title=f"–î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ: {title}",
            content=content,
            doc_type="achievement",
            metadata={
                "metrics": metrics,
                **(metadata or {})
            }
        )
    
    def train_index(self):
        """
        –û–±—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        """
        if not self.documents:
            print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return False
        
        print(f"üß† –û–±—É—á–µ–Ω–∏–µ RAG –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö...")
        
        # –û–±—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–µ—Ä–∞
        self.embedder.fit(self.documents)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        for doc in self.documents:
            doc.embedding = self.embedder.embed(doc.content)
        
        self.is_trained = True
        print("‚úÖ RAG –∏–Ω–¥–µ–∫—Å –æ–±—É—á–µ–Ω")
        
        return True
    
    def search(self, query: str, top_k: int = 5, doc_types: List[str] = None) -> List[Tuple[Document, float]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É
        """
        if not self.is_trained:
            print("‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –æ–±—É—á–µ–Ω")
            return []
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embedder.embed(query)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarities = []
        for doc in self.documents:
            if doc_types and doc.doc_type not in doc_types:
                continue
                
            if doc.embedding is not None:
                similarity = np.dot(query_embedding, doc.embedding)
                similarities.append((doc, similarity))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        return similarities[:top_k]
    
    def get_context_for_query(self, query: str, max_context_length: int = 1000) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        """
        results = self.search(query, top_k=3)
        
        if not results:
            return "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
        
        context_parts = []
        current_length = 0
        
        for doc, similarity in results:
            if current_length >= max_context_length:
                break
                
            context_part = f"[{doc.doc_type.upper()}] {doc.title}\n{doc.content[:200]}...\n"
            
            if current_length + len(context_part) <= max_context_length:
                context_parts.append(context_part)
                current_length += len(context_part)
        
        return "\n".join(context_parts)
    
    def save_index(self, filename: str = None):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        """
        if filename is None:
            filename = os.path.join(self.index_dir, f"rag_index_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl")
        
        index_data = {
            "documents": self.documents,
            "embedder_vocab": self.embedder.vocab,
            "embedder_idf": self.embedder.idf,
            "embedder_doc_count": self.embedder.doc_count,
            "is_trained": self.is_trained,
            "created_at": datetime.now().isoformat()
        }
        
        with open(filename, 'wb') as f:
            pickle.dump(index_data, f)
        
        print(f"üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}")
        return filename
    
    def load_index(self, filename: str):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞
        """
        try:
            with open(filename, 'rb') as f:
                index_data = pickle.load(f)
            
            self.documents = index_data["documents"]
            self.embedder.vocab = index_data["embedder_vocab"]
            self.embedder.idf = index_data["embedder_idf"]
            self.embedder.doc_count = index_data["embedder_doc_count"]
            self.is_trained = index_data["is_trained"]
            
            print(f"üìÇ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {filename}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            return False
    
    def get_stats(self) -> Dict:
        """
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞
        """
        if not self.documents:
            return {"message": "–ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç"}
        
        doc_types = {}
        for doc in self.documents:
            doc_types[doc.doc_type] = doc_types.get(doc.doc_type, 0) + 1
        
        return {
            "total_documents": len(self.documents),
            "document_types": doc_types,
            "is_trained": self.is_trained,
            "vocabulary_size": len(self.embedder.vocab) if self.embedder.vocab else 0
        }


class RAGManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä RAG –∏–Ω–¥–µ–∫—Å–∞
    """
    
    def __init__(self, index_dir: str = "data/rag_index"):
        self.index = RAGIndex(index_dir)
        self.is_initialized = False
        
    def initialize(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –º–µ–Ω–µ–¥–∂–µ—Ä–∞
        """
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
            index_files = [f for f in os.listdir(self.index.index_dir) if f.endswith('.pkl')]
            if index_files:
                latest_index = max(index_files)
                self.index.load_index(os.path.join(self.index.index_dir, latest_index))
                print("üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π RAG –∏–Ω–¥–µ–∫—Å")
            else:
                # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ —Å –±–∞–∑–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                self._create_default_index()
                print("üÜï –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π RAG –∏–Ω–¥–µ–∫—Å")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG: {e}")
            return False
    
    def _create_default_index(self):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å –±–∞–∑–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ AI –ú–ê–ì–ò–°–¢–†–ê
        """
        # –†–µ–∑—é–º–µ
        self.index.add_resume_section(
            "–û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã",
            """
            Senior ML Engineer —Å 5+ –ª–µ—Ç –æ–ø—ã—Ç–∞ –≤ production ML —Å–∏—Å—Ç–µ–º–∞—Ö.
            –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: LLM, –∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.
            –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–∑–∏—Ü–∏—è: Tech Lead –≤ AI —Å—Ç–∞—Ä—Ç–∞–ø–µ.
            """
        )
        
        self.index.add_resume_section(
            "–ù–∞–≤—ã–∫–∏",
            """
            Python, PyTorch, TensorFlow, FastAPI, Docker, Kubernetes.
            ML Ops: MLflow, Weights & Biases, Prometheus.
            Cloud: AWS, GCP, Azure.
            """
        )
        
        # –ü—Ä–æ–µ–∫—Ç—ã
        self.index.add_project(
            "Prometheus LLM",
            "Production LLM —Å–∏—Å—Ç–µ–º–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º",
            ["Python", "PyTorch", "FastAPI", "Docker"],
            [
                "p95 –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å 1.2s",
                "–ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å 45-60 —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫",
                "–ù–∏–∑–∫–∞—è —Å–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å –Ω–∞ 1000 —Ç–æ–∫–µ–Ω–æ–≤"
            ]
        )
        
        self.index.add_project(
            "AI Agent Framework",
            "–§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º",
            ["Python", "LangChain", "OpenAI API", "PostgreSQL"],
            [
                "–ü–æ–¥–¥–µ—Ä–∂–∫–∞ 10+ —Ç–∏–ø–æ–≤ –∞–≥–µ–Ω—Ç–æ–≤",
                "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á",
                "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ API"
            ]
        )
        
        # –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è
        self.index.add_achievement(
            "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞",
            "–°–Ω–∏–∂–µ–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ LLM –≤ 3 —Ä–∞–∑–∞",
            {
                "latency_improvement": "3x",
                "cost_reduction": "40%",
                "throughput_increase": "2.5x"
            }
        )
        
        # –û–±—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        self.index.train_index()
    
    def search_context(self, query: str, max_length: int = 1000) -> str:
        """
        –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        """
        if not self.is_initialized:
            return "RAG –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"
        
        return self.index.get_context_for_query(query, max_length)
    
    def add_personal_document(self, title: str, content: str, doc_type: str):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—á–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        if not self.is_initialized:
            return False
        
        doc_id = self.index.add_document(title, content, doc_type)
        self.index.train_index()  # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        return doc_id


# =============== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ===============

def test_rag_index():
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG –∏–Ω–¥–µ–∫—Å–∞
    """
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RAG –∏–Ω–¥–µ–∫—Å–∞...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
    rag_manager = RAGManager()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    if rag_manager.initialize():
        print("‚úÖ RAG –º–µ–Ω–µ–¥–∂–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        test_queries = [
            "–ö–∞–∫–∏–µ —É –º–µ–Ω—è –Ω–∞–≤—ã–∫–∏ –≤ Python?",
            "–†–∞—Å—Å–∫–∞–∂–∏ –æ –ø—Ä–æ–µ–∫—Ç–µ Prometheus",
            "–ö–∞–∫–∏–µ —É –º–µ–Ω—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏?",
            "–ö–∞–∫–æ–π —É –º–µ–Ω—è –æ–ø—ã—Ç —Å ML?"
        ]
        
        for query in test_queries:
            print(f"\nüîç –ó–∞–ø—Ä–æ—Å: {query}")
            context = rag_manager.search_context(query)
            print(f"üìÑ –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context[:200]}...")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = rag_manager.index.get_stats()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")
        
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAG")


if __name__ == "__main__":
    test_rag_index()
